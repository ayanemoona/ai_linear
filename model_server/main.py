# Render ìµœì í™” ë²„ì „ - ë©”ëª¨ë¦¬ ì ˆì•½ + ì•ˆì •ì„± ê°œì„ 
import os
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
import torch
import cv2
import numpy as np
import pandas as pd
from PIL import Image
from pathlib import Path
import shutil
from typing import List, Dict, Any
import uvicorn
import base64
import io
import time
import uuid
from datetime import datetime
import logging
import gc  # ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©

# í™˜ê²½ ë³€ìˆ˜
PORT = int(os.getenv("PORT", 8001))
ENVIRONMENT = os.getenv("ENVIRONMENT", "production")  # RenderëŠ” ê¸°ë³¸ production

# Render ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤í•œ ì„¤ì •
MEMORY_LIMIT_MODE = os.getenv("RENDER_MEMORY_LIMIT", "true").lower() == "true"

app = FastAPI(
    title="CCTV AI Analysis Server - Render Optimized",
    version="3.1.0",
    description="Render ìµœì í™”ëœ AI ì„œë²„"
)

# CORS ì„¤ì • (Render ë°°í¬ìš©)
# CORS ì„¤ì • (Render ë°°í¬ìš©) - ìˆ˜ì •ëœ ë¶€ë¶„ë§Œ
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://ai-linear.vercel.app",  # ğŸ†• Vercel í”„ë¡ íŠ¸ì—”ë“œ URL ì¶”ê°€
        "https://ai-linear-parkmoonas-projects.vercel.app",  # ğŸ†• ì¶”ê°€ Vercel ë„ë©”ì¸
        "http://localhost:3000",  # ë¡œì»¬ ê°œë°œìš©
        "http://localhost:5173",  # Vite ê°œë°œ ì„œë²„
        "*"  # ì„ì‹œë¡œ ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (ê°œë°œìš©)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„° ì €ì¥ í´ë” (Render ì„ì‹œ ë””ìŠ¤í¬ ì‚¬ìš©)
ROOT_DIR = Path('/tmp/cctv_data')  # Renderì—ì„œëŠ” /tmp ì‚¬ìš©
VIDEO_DIR = ROOT_DIR / 'videos'
CROP_DIR = ROOT_DIR / 'crops'

# í´ë” ìƒì„±
for folder in [ROOT_DIR, VIDEO_DIR, CROP_DIR]:
    folder.mkdir(parents=True, exist_ok=True)

# ì „ì—­ ë³€ìˆ˜ - Lazy Loadingìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
device = 'cpu'  # RenderëŠ” CPUë§Œ ì§€ì›
yolo_model = None
clip_model = None
clip_preprocess = None

# Render ìµœì í™” ì„¤ì • - ì‚¬ëŒ íƒì§€ ê°œì„ 
render_config = {
    "yolo_confidence_threshold": 0.3,  # ğŸ”¥ 0.6 â†’ 0.3 (ë” ë¯¼ê°í•˜ê²Œ)
    "yolo_model_size": "yolov8n",  # ê°€ì¥ ì‘ì€ ëª¨ë¸ ìœ ì§€
    "max_frames_per_video": 20,        # ğŸ”¥ 10 â†’ 20 (ë” ë§ì€ í”„ë ˆì„)
    "min_person_size": 30,             # ğŸ”¥ 80 â†’ 30 (ì‘ì€ ì‚¬ëŒë„ íƒì§€)
    "search_similarity_threshold": 0.1, # ğŸ”¥ 0.15 â†’ 0.1 (ë” ë¯¼ê°í•˜ê²Œ)
    "max_video_size_mb": 50            # ìµœëŒ€ ë¹„ë””ì˜¤ í¬ê¸° ì œí•œ ìœ ì§€
}

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class SearchRequest(BaseModel):
    query: str
    k: int = 3  # ì ì€ ê²°ê³¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

class SearchResult(BaseModel):
    image_path: str
    caption: str
    score: float
    image_base64: str = ""

# ë¶„ì„ëœ ë°ì´í„° ì €ì¥ (ë©”ëª¨ë¦¬ ì œí•œ)
analyzed_data = {
    "embeddings": None,
    "image_paths": [],
    "captions": [],
    "images": [],
    "last_analysis": None
}

def load_yolo_model():
    """YOLO ëª¨ë¸ ì§€ì—° ë¡œë”©"""
    global yolo_model
    if yolo_model is None:
        try:
            print("ğŸ“¦ YOLO ëª¨ë¸ ë¡œë”©...")
            from ultralytics import YOLO
            yolo_model = YOLO(f"{render_config['yolo_model_size']}.pt")
            print("âœ… YOLO ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ YOLO ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    return yolo_model

def load_clip_model():
    """CLIP ëª¨ë¸ ì§€ì—° ë¡œë”©"""
    global clip_model, clip_preprocess
    if clip_model is None:
        try:
            print("ğŸ§  CLIP ëª¨ë¸ ë¡œë”©...")
            import clip
            clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
            print("âœ… CLIP ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ CLIP ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    return clip_model, clip_preprocess

def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ - ëª¨ë¸ì€ í•„ìš”í•  ë•Œ ë¡œë”©"""
    print("ğŸš€ Render ìµœì í™” AI ì„œë²„ ì‹œì‘!")
    print(f"ğŸŒ í™˜ê²½: {ENVIRONMENT}")
    print(f"ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì œí•œ ëª¨ë“œ: {MEMORY_LIMIT_MODE}")
    print("ğŸ“ ëª¨ë¸ì€ í•„ìš”í•  ë•Œ ë¡œë”©ë©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ì ˆì•½)")

@app.get("/")
async def root():
    """ë©”ì¸ í˜ì´ì§€"""
    return {
        "service": "CCTV AI Analysis Server - Render Optimized",
        "version": "3.1.0",
        "environment": ENVIRONMENT,
        "memory_limit_mode": MEMORY_LIMIT_MODE,
        "models_loaded": {
            "yolo": yolo_model is not None,
            "clip": clip_model is not None
        },
        "config": render_config,
        "message": "âœ… Render ìµœì í™” ì„œë²„ ì‹¤í–‰ì¤‘! ğŸ’ª"
    }

@app.get("/health")
async def health_check():
    """ìƒì„¸ í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "platform": "Render",
        "models_loaded": {
            "yolo": yolo_model is not None,
            "clip": clip_model is not None
        },
        "device": device,
        "analyzed_data_count": len(analyzed_data["image_paths"]),
        "memory_mode": "optimized" if MEMORY_LIMIT_MODE else "normal"
    }

def detect_persons_in_video_optimized(video_path: Path, max_frames: int = 20):
    """ê°œì„ ëœ ì‚¬ëŒ íƒì§€ - ë” ë¯¼ê°í•˜ê²Œ"""
    
    model = load_yolo_model()
    print(f"ğŸ¬ ê°œì„ ëœ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘: {video_path.name}")
    
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    detected_persons = []
    frame_count = 0
    
    # ğŸ”¥ í”„ë ˆì„ ê°„ê²©ì„ ë” ì‘ê²Œ (ë” ë§ì€ í”„ë ˆì„ ë¶„ì„)
    frame_interval = max(2, total_frames // max_frames)  # ìµœì†Œ 2í”„ë ˆì„ë§ˆë‹¤
    
    print(f"ğŸ“Š ì´ í”„ë ˆì„: {total_frames}, ë¶„ì„í•  í”„ë ˆì„: {min(max_frames, total_frames // frame_interval)}")
    print(f"ğŸ¯ íƒì§€ ì„ê³„ê°’: {render_config['yolo_confidence_threshold']}")
    print(f"ğŸ“ ìµœì†Œ ì‚¬ëŒ í¬ê¸°: {render_config['min_person_size']}px")
    
    try:
        while len(detected_persons) < 30:  # ğŸ”¥ 20 â†’ 30ê°œë¡œ ì¦ê°€
            ret, frame = cap.read()
            if not ret:
                break
                
            frame_count += 1
            
            if frame_count % frame_interval != 0:
                continue
            
            # í”„ë ˆì„ í¬ê¸°ëŠ” ìœ ì§€ (ë„ˆë¬´ ì‘ê²Œ í•˜ë©´ íƒì§€ ì•ˆë¨)
            height, width = frame.shape[:2]
            if width > 800:  # ğŸ”¥ 640 â†’ 800ìœ¼ë¡œ ì¦ê°€
                scale = 800 / width
                new_width = int(width * scale)
                new_height = int(height * scale)
                frame = cv2.resize(frame, (new_width, new_height))
            
            try:
                # YOLO ì¶”ë¡  - ë” ë¯¼ê°í•˜ê²Œ
                results = model(frame, classes=[0], verbose=False, conf=render_config['yolo_confidence_threshold'])
                
                for r in results:
                    boxes = r.boxes
                    if boxes is not None:
                        print(f"ğŸ” í”„ë ˆì„ {frame_count}ì—ì„œ {len(boxes)}ê°œ ê°ì²´ ê°ì§€")
                        
                        for box in boxes:
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            confidence = box.conf[0].cpu().numpy()
                            
                            print(f"   ğŸ“¦ ë°•ìŠ¤: ({x1:.0f},{y1:.0f},{x2:.0f},{y2:.0f}), ì‹ ë¢°ë„: {confidence:.2f}")
                            
                            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                            
                            width_box = x2 - x1
                            height_box = y2 - y1
                            
                            print(f"   ğŸ“ ë°•ìŠ¤ í¬ê¸°: {width_box}x{height_box}")
                            
                            # ğŸ”¥ í¬ê¸° ì¡°ê±´ ì™„í™”
                            if width_box > render_config["min_person_size"] and height_box > render_config["min_person_size"]:
                                person_img = frame[y1:y2, x1:x2]
                                person_pil = Image.fromarray(cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB))
                                
                                # ì´ë¯¸ì§€ í¬ê¸° ì œí•œ ì™„í™”
                                if person_pil.size[0] > 400 or person_pil.size[1] > 400:
                                    person_pil.thumbnail((400, 400), Image.Resampling.LANCZOS)
                                
                                filename = f"{video_path.stem}_f{frame_count}_p{len(detected_persons)}.jpg"
                                crop_path = CROP_DIR / filename
                                person_pil.save(crop_path, quality=90)  # ğŸ”¥ 85 â†’ 90 í’ˆì§ˆ í–¥ìƒ
                                
                                detected_persons.append({
                                    "íŒŒì¼ê²½ë¡œ": str(crop_path),
                                    "í”„ë ˆì„ë²ˆí˜¸": frame_count,
                                    "ì‹ ë¢°ë„": float(confidence),
                                    "ë°•ìŠ¤ì¢Œí‘œ": [x1, y1, x2, y2],
                                    "ì´ë¯¸ì§€": person_pil
                                })
                                
                                print(f"âœ… ì‚¬ëŒ íƒì§€ ì„±ê³µ: í”„ë ˆì„ {frame_count}, ì‹ ë¢°ë„ {confidence:.2f}")
                            else:
                                print(f"âŒ í¬ê¸° ë¶€ì¡±: {width_box}x{height_box} < {render_config['min_person_size']}")
                    else:
                        print(f"ğŸ” í”„ë ˆì„ {frame_count}ì—ì„œ ê°ì²´ ì—†ìŒ")
                        
                # ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” ë” ìì£¼
                if frame_count % 5 == 0:
                    cleanup_memory()
            
            except Exception as e:
                print(f"âš ï¸ í”„ë ˆì„ {frame_count} ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue
    
    finally:
        cap.release()
        cleanup_memory()
    
    print(f"ğŸ‰ ë¶„ì„ ì™„ë£Œ: {len(detected_persons)}ëª…ì˜ ì‚¬ëŒ íƒì§€")
    return detected_persons

def generate_clip_embeddings_optimized(detected_persons):
    """ë©”ëª¨ë¦¬ ìµœì í™”ëœ CLIP ì„ë² ë”© ìƒì„±"""
    
    clip_model, clip_preprocess = load_clip_model()
    print("ğŸ§  CLIP ì„ë² ë”© ìƒì„± ì¤‘...")
    
    embeddings = []
    image_paths = []
    captions = []
    images = []
    
    for i, person in enumerate(detected_persons):
        try:
            image = person["ì´ë¯¸ì§€"]
            image_input = clip_preprocess(image).unsqueeze(0).to(device)
            
            with torch.no_grad():
                image_features = clip_model.encode_image(image_input)
                image_features = image_features.cpu().numpy()[0]
            
            embeddings.append(image_features)
            image_paths.append(person["íŒŒì¼ê²½ë¡œ"])
            
            caption = f"í”„ë ˆì„ {person['í”„ë ˆì„ë²ˆí˜¸']}ì—ì„œ íƒì§€ëœ ì¸ë¬¼ (ì‹ ë¢°ë„: {person['ì‹ ë¢°ë„']:.1%})"
            captions.append(caption)
            
            # Base64 ì¸ì½”ë”© (í¬ê¸° ì œí•œ)
            buffered = io.BytesIO()
            image.save(buffered, format="JPEG", quality=80)
            img_base64 = base64.b64encode(buffered.getvalue()).decode()
            images.append(img_base64)
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % 3 == 0:
                cleanup_memory()
            
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ {i}: {e}")
            continue
    
    cleanup_memory()
    print(f"âœ… {len(embeddings)}ê°œì˜ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    return {
        "embeddings": np.vstack(embeddings) if embeddings else None,
        "image_paths": image_paths,
        "captions": captions,
        "images": images
    }

@app.post("/upload-video")
async def upload_video(file: UploadFile = File(...)):
    """Render ìµœì í™”ëœ ì˜ìƒ ì—…ë¡œë“œ ë° AI ë¶„ì„"""
    
    print(f"ğŸ“¹ Render AI ë¶„ì„ ì‹œì‘: {file.filename}")
    
    # íŒŒì¼ í¬ê¸° ì²´í¬
    file_size = 0
    content = await file.read()
    file_size = len(content) / 1024 / 1024  # MB
    
    if file_size > render_config["max_video_size_mb"]:
        raise HTTPException(
            status_code=413, 
            detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ {render_config['max_video_size_mb']}MBê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤."
        )
    
    if not file.filename.lower().endswith(('.mp4', '.avi', '.mov')):
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹")
    
    try:
        # íŒŒì¼ ì €ì¥
        video_path = VIDEO_DIR / file.filename
        with open(video_path, "wb") as buffer:
            buffer.write(content)
        
        print(f"ğŸ“ íŒŒì¼ ì €ì¥ë¨: {file_size:.1f}MB")
        
        # ì‚¬ëŒ íƒì§€ (ìµœì í™”ë¨)
        detected_persons = detect_persons_in_video_optimized(
            video_path, 
            render_config["max_frames_per_video"]
        )
        
        if not detected_persons:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            video_path.unlink(missing_ok=True)
            return {
                "status": "success",
                "message": "ë¶„ì„ ì™„ë£Œ, íƒì§€ëœ ì‚¬ëŒ ì—†ìŒ",
                "total_crops": 0
            }
        
        # CLIP ì„ë² ë”© ìƒì„± (ìµœì í™”ë¨)
        embedding_data = generate_clip_embeddings_optimized(detected_persons)
        
        # ê¸€ë¡œë²Œ ë°ì´í„° ì—…ë°ì´íŠ¸
        global analyzed_data
        analyzed_data = embedding_data
        analyzed_data["last_analysis"] = datetime.now().isoformat()
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ì‚­ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        video_path.unlink(missing_ok=True)
        
        cleanup_memory()
        
        return {
            "status": "success",
            "message": f"'{file.filename}' Render AI ë¶„ì„ ì™„ë£Œ! ğŸ‰",
            "total_crops": len(detected_persons),
            "file_size_mb": round(file_size, 1),
            "ë¶„ì„ê²°ê³¼": f"{len(detected_persons)}ëª…ì˜ ì‹¤ì œ ì¸ë¬¼ì´ AIë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        # ì˜¤ë¥˜ ì‹œ ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if 'video_path' in locals():
            video_path.unlink(missing_ok=True)
        
        cleanup_memory()
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/search", response_model=List[SearchResult])
async def search_persons(request: SearchRequest):
    """Render ìµœì í™”ëœ CLIP ê¸°ë°˜ ê²€ìƒ‰"""
    
    if analyzed_data["embeddings"] is None:
        raise HTTPException(status_code=404, detail="ë¨¼ì € ì˜ìƒì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”")
    
    try:
        # CLIP ëª¨ë¸ ë¡œë“œ
        clip_model, _ = load_clip_model()
        
        # CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        import clip
        text_input = clip.tokenize([request.query]).to(device)
        
        with torch.no_grad():
            text_features = clip_model.encode_text(text_input)
            text_features = text_features.cpu().numpy()[0]
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        image_embeddings = analyzed_data["embeddings"]
        similarities = np.dot(image_embeddings, text_features) / (
            np.linalg.norm(image_embeddings, axis=1) * np.linalg.norm(text_features)
        )
        
        # ìƒìœ„ ê²°ê³¼ ì„ íƒ (ì œí•œë¨)
        top_indices = np.argsort(-similarities)[:min(request.k, 5)]
        
        results = []
        for idx in top_indices:
            similarity = similarities[idx]
            if similarity > render_config["search_similarity_threshold"]:
                results.append(SearchResult(
                    image_path=analyzed_data["image_paths"][idx],
                    caption=analyzed_data["captions"][idx],
                    score=float(similarity),
                    image_base64=analyzed_data["images"][idx]
                ))
        
        cleanup_memory()
        print(f"ğŸ” Render AI ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results
        
    except Exception as e:
        cleanup_memory()
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")

@app.get("/stats")
async def get_stats():
    """Render ì„œë²„ í†µê³„"""
    return {
        "ì„œë²„_ìƒíƒœ": "Render ìµœì í™” ì‹¤í–‰ì¤‘",
        "í”Œë«í¼": "Render",
        "ëª¨ë¸_ë¡œë”©": {
            "YOLO": yolo_model is not None,
            "CLIP": clip_model is not None
        },
        "ì„¤ì •": render_config,
        "ë¶„ì„ëœ_ë°ì´í„°": len(analyzed_data["image_paths"]),
        "ë§ˆì§€ë§‰_ë¶„ì„": analyzed_data.get("last_analysis", "ì—†ìŒ"),
        "ë©”ëª¨ë¦¬_ëª¨ë“œ": "ìµœì í™”ë¨" if MEMORY_LIMIT_MODE else "ì¼ë°˜"
    }

@app.get("/image/{filename}")
async def get_image(filename: str):
    """ì´ë¯¸ì§€ íŒŒì¼ ë°˜í™˜"""
    image_path = CROP_DIR / filename
    if image_path.exists():
        return FileResponse(image_path)
    else:
        raise HTTPException(status_code=404, detail="ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

# ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ Render ìµœì í™” AI ì„œë²„ ì‹œì‘!")
    print("ğŸ’ª 7ë‹¬ëŸ¬ì˜ ê°€ì¹˜ë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤!")
    print("=" * 50)
    print(f"ğŸ“ ì„œë²„: http://localhost:{PORT}")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìµœì í™”: {MEMORY_LIMIT_MODE}")
    print("=" * 50)
    
    uvicorn.run(app, host="0.0.0.0", port=PORT)